# -*- coding: utf-8 -*-
"""
è¥¿è¯è¯äºŒçŸ¥è¯†ç‚¹æå–è„šæœ¬ V2
æ›´ç²¾ç¡®åœ°ä»PDFè§£æJSONä¸­æå–ç»“æ„åŒ–çŸ¥è¯†ç‚¹
"""

import json
import re
from typing import Dict, List, Optional
from dataclasses import dataclass, field

@dataclass
class KnowledgePoint:
    """çŸ¥è¯†ç‚¹"""
    id: str
    title: str
    content: str
    point_type: str = ""  # é€‚åº”è¯/ç¦å¿Œ/ä¸è‰¯ååº”/ç”¨æ³•ç”¨é‡/æ³¨æ„äº‹é¡¹/ç›¸äº’ä½œç”¨
    drug_name: str = ""
    importance: int = 3
    memory_tips: str = ""

@dataclass
class Section:
    """å°èŠ‚"""
    id: str
    title: str
    points: List[Dict] = field(default_factory=list)

@dataclass
class Chapter:
    """ç« èŠ‚"""
    id: str
    title: str
    sections: List[Section] = field(default_factory=list)


class KnowledgeExtractorV2:
    """çŸ¥è¯†ç‚¹æå–å™¨V2"""
    
    def __init__(self):
        self.chapters = []
        self.all_text_by_page = {}
        
        # æ­£åˆ™æ¨¡å¼
        self.patterns = {
            'chapter': re.compile(r'ç¬¬([ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+)ç« \s*(.+?)(?:\s*//|$)'),
            'section': re.compile(r'ç¬¬([ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+)èŠ‚\s*(.+?)(?:\s*\d+|$)'),
            'point': re.compile(r'è€ƒç‚¹(\d+)\s*(.+)'),
            'drug_eval': re.compile(r'(.+?)çš„ä¸´åºŠç”¨è¯è¯„ä»·'),
            'memory': re.compile(r'ã€æ¶¦å¾·å·§è®°ã€‘(.+?)(?=è€ƒç‚¹|$)', re.DOTALL),
        }
        
        # çŸ¥è¯†ç‚¹ç±»å‹å…³é”®è¯
        self.type_patterns = {
            'é€‚åº”è¯': [r'é€‚åº”è¯', r'é€‚åº”ç—‡', r'ç”¨äºæ²»ç–—', r'ä¸»è¦ç”¨äº', r'ä¸´åºŠç”¨äº'],
            'ç¦å¿Œ': [r'ç¦å¿Œ', r'ç¦ç”¨äº', r'ä¸å®œç”¨äº', r'æ…ç”¨'],
            'ä¸è‰¯ååº”': [r'ä¸è‰¯ååº”', r'å‰¯ä½œç”¨', r'æ¯’æ€§ååº”', r'å¸¸è§ä¸è‰¯'],
            'ç”¨æ³•ç”¨é‡': [r'ç”¨æ³•', r'ç”¨é‡', r'å‰‚é‡', r'ç»™è¯é€”å¾„', r'å£æœ', r'é™è„‰'],
            'æ³¨æ„äº‹é¡¹': [r'æ³¨æ„äº‹é¡¹', r'ä¸´åºŠåº”ç”¨æ³¨æ„', r'ä½¿ç”¨æ³¨æ„', r'è­¦å‘Š'],
            'ç›¸äº’ä½œç”¨': [r'ç›¸äº’ä½œç”¨', r'è¯ç‰©ç›¸äº’', r'é…ä¼', r'è”åˆç”¨è¯'],
            'ä½œç”¨æœºåˆ¶': [r'ä½œç”¨æœºåˆ¶', r'è¯ç†ä½œç”¨', r'æœºåˆ¶', r'é€šè¿‡æŠ‘åˆ¶', r'é€šè¿‡é˜»æ–­'],
        }
        
        # é‡è¦æ€§å…³é”®è¯
        self.importance_keywords = {
            5: ['é¦–é€‰', 'ä¸€çº¿è¯ç‰©', 'é‡‘æ ‡å‡†', 'æœ€å¸¸ç”¨'],
            4: ['ç¦å¿Œ', 'ç¦ç”¨', 'ä¸¥é‡', 'è‡´æ­»', 'ç‰¹åˆ«æ³¨æ„', 'ç›¸äº’ä½œç”¨'],
            3: ['å¸¸ç”¨', 'ä¸»è¦', 'é‡è¦'],
        }
        
        # ä¸­æ–‡æ•°å­—æ˜ å°„
        self.cn_num = {
            'ä¸€': 1, 'äºŒ': 2, 'ä¸‰': 3, 'å››': 4, 'äº”': 5,
            'å…­': 6, 'ä¸ƒ': 7, 'å…«': 8, 'ä¹': 9, 'å': 10,
            'åä¸€': 11, 'åäºŒ': 12, 'åä¸‰': 13, 'åå››': 14, 'åäº”': 15,
            'åå…­': 16, 'åä¸ƒ': 17, 'åå…«': 18, 'åä¹': 19, 'äºŒå': 20
        }
    
    def extract_from_json(self, json_path: str) -> Dict:
        """ä»JSONæå–çŸ¥è¯†ç‚¹"""
        with open(json_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        # 1. æå–æ‰€æœ‰é¡µé¢æ–‡æœ¬
        self._extract_all_text(data)
        
        # 2. è§£æç›®å½•ç»“æ„ï¼ˆä»å‰å‡ é¡µï¼‰
        self._parse_toc()
        
        # 3. è§£ææ­£æ–‡å†…å®¹
        self._parse_content()
        
        # 4. æ„å»ºçŸ¥è¯†æ ‘
        return self._build_tree()
    
    def _extract_all_text(self, data: Dict):
        """æå–æ‰€æœ‰é¡µé¢æ–‡æœ¬"""
        for page in data.get('pdf_info', []):
            page_idx = page.get('page_idx', 0)
            texts = []
            
            for block in page.get('para_blocks', []):
                text = self._get_block_text(block)
                if text.strip():
                    texts.append({
                        'text': text.strip(),
                        'type': block.get('type', 'text')
                    })
            
            self.all_text_by_page[page_idx] = texts
    
    def _get_block_text(self, block: Dict) -> str:
        """è·å–å—æ–‡æœ¬"""
        parts = []
        
        if 'lines' in block:
            for line in block['lines']:
                for span in line.get('spans', []):
                    if 'content' in span:
                        parts.append(span['content'])
        
        if 'blocks' in block:
            for sub in block['blocks']:
                parts.append(self._get_block_text(sub))
        
        return ' '.join(parts)
    
    def _parse_toc(self):
        """è§£æç›®å½•"""
        # ç›®å½•é€šå¸¸åœ¨å‰5é¡µ
        toc_text = ""
        for page_idx in range(min(6, len(self.all_text_by_page))):
            for item in self.all_text_by_page.get(page_idx, []):
                toc_text += item['text'] + "\n"
        
        # æå–ç« èŠ‚
        for match in self.patterns['chapter'].finditer(toc_text):
            cn_num = match.group(1)
            title = match.group(2).strip()
            chapter_id = str(self.cn_num.get(cn_num, len(self.chapters) + 1))
            
            # é¿å…é‡å¤
            if not any(c['id'] == chapter_id for c in self.chapters):
                self.chapters.append({
                    'id': chapter_id,
                    'title': title,
                    'sections': []
                })
        
        print(f"ä»ç›®å½•è§£æåˆ° {len(self.chapters)} ä¸ªç« èŠ‚")
    
    def _parse_content(self):
        """è§£ææ­£æ–‡å†…å®¹"""
        current_chapter = None
        current_section = None
        current_point = None
        content_buffer = []
        
        # ä»ç¬¬6é¡µå¼€å§‹æ˜¯æ­£æ–‡
        for page_idx in range(6, len(self.all_text_by_page)):
            for item in self.all_text_by_page.get(page_idx, []):
                text = item['text']
                
                # æ£€æµ‹å°èŠ‚æ ‡é¢˜
                section_match = self.patterns['section'].search(text)
                if section_match:
                    # ä¿å­˜ä¹‹å‰çš„å†…å®¹
                    if current_point and content_buffer:
                        current_point['content'] = '\n'.join(content_buffer)
                        self._classify_point(current_point)
                        content_buffer = []
                    
                    cn_num = section_match.group(1)
                    section_title = section_match.group(2).strip()
                    section_num = self.cn_num.get(cn_num, 1)
                    
                    # ç¡®å®šæ‰€å±ç« èŠ‚
                    current_chapter = self._find_chapter_for_section(section_title)
                    if current_chapter:
                        section_id = f"{current_chapter['id']}.{section_num}"
                        current_section = {
                            'id': section_id,
                            'title': section_title,
                            'points': []
                        }
                        current_chapter['sections'].append(current_section)
                        current_point = None
                    continue
                
                # æ£€æµ‹è€ƒç‚¹
                point_match = self.patterns['point'].search(text)
                if point_match and current_section:
                    # ä¿å­˜ä¹‹å‰çš„å†…å®¹
                    if current_point and content_buffer:
                        current_point['content'] = '\n'.join(content_buffer)
                        self._classify_point(current_point)
                        content_buffer = []
                    
                    point_num = point_match.group(1)
                    point_title = point_match.group(2).strip()
                    point_id = f"{current_section['id']}.{point_num}"
                    
                    current_point = {
                        'id': point_id,
                        'title': point_title,
                        'content': '',
                        'point_type': '',
                        'drug_name': '',
                        'importance': 3,
                        'memory_tips': ''
                    }
                    current_section['points'].append(current_point)
                    
                    # æ£€æµ‹è¯ç‰©åç§°
                    drug_match = self.patterns['drug_eval'].search(point_title)
                    if drug_match:
                        current_point['drug_name'] = drug_match.group(1)
                    continue
                
                # æ£€æµ‹è®°å¿†å£è¯€
                memory_match = self.patterns['memory'].search(text)
                if memory_match and current_point:
                    current_point['memory_tips'] = memory_match.group(1).strip()
                
                # æ”¶é›†å†…å®¹
                if current_point and text.strip():
                    content_buffer.append(text.strip())
        
        # ä¿å­˜æœ€åçš„å†…å®¹
        if current_point and content_buffer:
            current_point['content'] = '\n'.join(content_buffer)
            self._classify_point(current_point)
    
    def _find_chapter_for_section(self, section_title: str) -> Optional[Dict]:
        """æ ¹æ®å°èŠ‚æ ‡é¢˜æ‰¾åˆ°æ‰€å±ç« èŠ‚"""
        # å…³é”®è¯æ˜ å°„
        keyword_chapter = {
            'è§£çƒ­': '2', 'é•‡ç—›': '2', 'æŠ—ç‚': '2', 'æŠ—é£æ¹¿': '2', 'æŠ—ç—›é£': '2',
            'é•‡å’³': '3', 'ç¥›ç—°': '3', 'å¹³å–˜': '3', 'å‘¼å¸': '3', 'è‚ºçº¤ç»´åŒ–': '3',
            'æŠ‘é…¸': '4', 'èƒƒ': '4', 'è‚ ': '4', 'æ¶ˆåŒ–': '4', 'è‚èƒ†': '4', 'æ­¢å': '4',
            'å¿ƒå¾‹': '5', 'é«˜è¡€å‹': '5', 'è¡€è„‚': '5', 'å¿ƒç»ç—›': '5', 'å¿ƒåŠ›è¡°ç«­': '5',
            'è¡€æ “': '6', 'å‡ºè¡€': '6', 'è´«è¡€': '6', 'ç™½ç»†èƒ': '6', 'éª¨é«“': '6',
            'åˆ©å°¿': '7', 'å‰åˆ—è…º': '7', 'å‹ƒèµ·': '7', 'è†€èƒ±': '7',
            'å‚ä½“': '8', 'ç³–çš®è´¨': '8', 'ç”²çŠ¶è…º': '8', 'èƒ°å²›ç´ ': '8', 'è¡€ç³–': '8', 'éª¨ä»£è°¢': '8', 'æ€§æ¿€ç´ ': '8',
            'æŠ—èŒ': '9', 'é’éœ‰ç´ ': '9', 'å¤´å­¢': '9', 'æŠ—æ„ŸæŸ“': '9', 'æŠ—ç—…æ¯’': '9', 'æŠ—çœŸèŒ': '9',
            'æŠ—è‚¿ç˜¤': '10', 'DNA': '10', 'æŠ—ä»£è°¢': '10',
            'ç”µè§£è´¨': '11', 'è¥å…»': '11', 'ç»´ç”Ÿç´ ': '11',
            'çœ¼ç§‘': '12', 'è€³é¼»': '12', 'å£è…”': '12',
            'çš®è‚¤': '13', 'æŠ—è¿‡æ•': '13',
            'é•‡é™': '1', 'å‚¬çœ ': '1', 'ä¸­æ¢': '1', 'ç²¾ç¥': '1',
        }
        
        for keyword, chapter_id in keyword_chapter.items():
            if keyword in section_title:
                for chapter in self.chapters:
                    if chapter['id'] == chapter_id:
                        return chapter
        
        # é»˜è®¤è¿”å›æœ€åä¸€ä¸ªç« èŠ‚
        return self.chapters[-1] if self.chapters else None
    
    def _classify_point(self, point: Dict):
        """åˆ†ç±»çŸ¥è¯†ç‚¹ç±»å‹"""
        content = point.get('content', '') + point.get('title', '')
        
        # æ£€æµ‹ç±»å‹
        for ptype, patterns in self.type_patterns.items():
            for pattern in patterns:
                if re.search(pattern, content):
                    point['point_type'] = ptype
                    break
            if point['point_type']:
                break
        
        # è®¡ç®—é‡è¦æ€§
        for level, keywords in self.importance_keywords.items():
            for keyword in keywords:
                if keyword in content:
                    point['importance'] = max(point['importance'], level)
                    break
    
    def _build_tree(self) -> Dict:
        """æ„å»ºçŸ¥è¯†æ ‘"""
        # æ¸…ç†ç©ºç« èŠ‚
        valid_chapters = []
        for chapter in self.chapters:
            valid_sections = [s for s in chapter['sections'] if s['points']]
            if valid_sections:
                chapter['sections'] = valid_sections
                valid_chapters.append(chapter)
        
        tree = {
            'subject': 'è¯å­¦ä¸“ä¸šçŸ¥è¯†ï¼ˆäºŒï¼‰',
            'subject_code': 'xiyao_yaoxue_er',
            'total_chapters': len(valid_chapters),
            'total_sections': sum(len(c['sections']) for c in valid_chapters),
            'total_points': sum(len(s['points']) for c in valid_chapters for s in c['sections']),
            'chapters': valid_chapters
        }
        
        return tree
    
    def export_to_json(self, tree: Dict, output_path: str):
        """å¯¼å‡ºJSON"""
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(tree, f, ensure_ascii=False, indent=2)
        print(f"å·²å¯¼å‡ºåˆ°: {output_path}")
    
    def export_for_database(self, tree: Dict, output_path: str):
        """å¯¼å‡ºæ•°æ®åº“æ ¼å¼"""
        records = []
        
        for chapter in tree['chapters']:
            # ç« èŠ‚è®°å½•
            records.append({
                'type': 'chapter',
                'id': f"xiyao_er_{chapter['id']}",
                'code': chapter['id'],
                'title': chapter['title'],
                'parent_id': None,
                'subject_code': tree['subject_code'],
                'level': 1
            })
            
            for section in chapter['sections']:
                # å°èŠ‚è®°å½•
                records.append({
                    'type': 'section',
                    'id': f"xiyao_er_{section['id']}",
                    'code': section['id'],
                    'title': section['title'],
                    'parent_id': f"xiyao_er_{chapter['id']}",
                    'subject_code': tree['subject_code'],
                    'level': 2
                })
                
                for point in section['points']:
                    # çŸ¥è¯†ç‚¹è®°å½•
                    records.append({
                        'type': 'knowledge_point',
                        'id': f"xiyao_er_{point['id']}",
                        'code': point['id'],
                        'title': point['title'],
                        'content': point['content'],
                        'point_type': point['point_type'],
                        'drug_name': point['drug_name'],
                        'importance': point['importance'],
                        'memory_tips': point['memory_tips'],
                        'parent_id': f"xiyao_er_{section['id']}",
                        'subject_code': tree['subject_code'],
                        'level': 3
                    })
        
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(records, f, ensure_ascii=False, indent=2)
        
        print(f"å·²å¯¼å‡ºæ•°æ®åº“è®°å½•: {output_path}")
        print(f"æ€»è®°å½•æ•°: {len(records)}")
        return records
    
    def print_summary(self, tree: Dict):
        """æ‰“å°æ‘˜è¦"""
        print("\n" + "="*60)
        print(f"ğŸ“š {tree['subject']}")
        print("="*60)
        print(f"ç« èŠ‚æ•°: {tree['total_chapters']}")
        print(f"å°èŠ‚æ•°: {tree['total_sections']}")
        print(f"çŸ¥è¯†ç‚¹æ•°: {tree['total_points']}")
        
        print("\nğŸ“– ç« èŠ‚ç›®å½•:")
        for chapter in tree['chapters']:
            print(f"\nç¬¬{chapter['id']}ç«  {chapter['title']}")
            for section in chapter['sections']:
                print(f"  â”œâ”€ {section['id']} {section['title']}")
                print(f"  â”‚   â””â”€ çŸ¥è¯†ç‚¹: {len(section['points'])}ä¸ª")
                
                # æ˜¾ç¤ºå‰3ä¸ªçŸ¥è¯†ç‚¹
                for point in section['points'][:3]:
                    importance_stars = 'â˜…' * point['importance'] + 'â˜†' * (5 - point['importance'])
                    print(f"  â”‚       â€¢ {point['title'][:30]}... [{importance_stars}]")
                if len(section['points']) > 3:
                    print(f"  â”‚       ... è¿˜æœ‰ {len(section['points']) - 3} ä¸ªçŸ¥è¯†ç‚¹")


def main():
    input_path = "shuju/è¥¿è¯è¯äºŒ1-50é¡µ.json"
    tree_output = "shuju/è¥¿è¯è¯äºŒ_çŸ¥è¯†æ ‘_v2.json"
    db_output = "shuju/è¥¿è¯è¯äºŒ_æ•°æ®åº“è®°å½•_v2.json"
    
    extractor = KnowledgeExtractorV2()
    
    print("å¼€å§‹æå–çŸ¥è¯†ç‚¹...")
    tree = extractor.extract_from_json(input_path)
    
    extractor.print_summary(tree)
    extractor.export_to_json(tree, tree_output)
    extractor.export_for_database(tree, db_output)


if __name__ == "__main__":
    main()
