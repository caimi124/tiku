#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ä»è¥¿è¯è¯äºŒPDFè§£æJSONä¸­æå–çŸ¥è¯†ç‚¹ï¼Œæ„å»ºæ ‘çŠ¶çŸ¥è¯†ä½“ç³»
"""

import json
import re
from typing import Dict, List, Optional, Any
from collections import defaultdict

class KnowledgeTreeExtractor:
    def __init__(self):
        # ç« èŠ‚åŒ¹é…æ¨¡å¼
        self.chapter_pattern = re.compile(r'ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+ç« \s+([^/]+)')
        self.section_pattern = re.compile(r'ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+èŠ‚\s+([^\n]+)')
        
        # çŸ¥è¯†ç‚¹ç±»å‹å…³é”®è¯
        self.knowledge_types = {
            'é€‚åº”è¯': ['é€‚åº”è¯', 'é€‚åº”ç—‡', 'ç”¨äº', 'é€‚ç”¨äº', 'æ²»ç–—'],
            'ç¦å¿Œ': ['ç¦å¿Œ', 'ç¦ç”¨', 'æ…ç”¨', 'ç¦æ­¢', 'ä¸å®œ'],
            'ä¸è‰¯ååº”': ['ä¸è‰¯ååº”', 'å‰¯ä½œç”¨', 'æ¯’æ€§', 'å¸¸è§', 'å¶è§', 'ç½•è§'],
            'ä¸´åºŠåº”ç”¨': ['ä¸´åºŠåº”ç”¨', 'ç”¨è¯æ³¨æ„', 'æ³¨æ„äº‹é¡¹', 'ç”¨è¯æŒ‡å¯¼'],
            'ç”¨æ³•ç”¨é‡': ['ç”¨æ³•', 'ç”¨é‡', 'å‰‚é‡', 'ç»™è¯', 'æœç”¨'],
            'è¯ç‰©ç›¸äº’ä½œç”¨': ['ç›¸äº’ä½œç”¨', 'åˆç”¨', 'è”ç”¨', 'é…ä¼'],
            'ä½œç”¨æœºåˆ¶': ['ä½œç”¨æœºåˆ¶', 'è¯ç†ä½œç”¨', 'ä½œç”¨ç‰¹ç‚¹', 'æœºåˆ¶'],
            'è®°å¿†å£è¯€': ['æ¶¦å¾·å·§è®°', 'è®°å¿†å£è¯€', 'å·§è®°', 'å£è¯€']
        }
        
        # é«˜é¢‘è€ƒç‚¹å…³é”®è¯
        self.high_freq_keywords = [
            'é¦–é€‰', 'ä¸€çº¿', 'é‡‘æ ‡å‡†', 'æœ€å¸¸ç”¨', 'ä¸»è¦', 'é‡è¦',
            'ä¸¥é‡', 'ç¦å¿Œ', 'ç¦ç”¨', 'ç‰¹åˆ«æ³¨æ„', 'é‡ç‚¹',
            'ç›¸äº’ä½œç”¨', 'é…ä¼ç¦å¿Œ', 'å¸¸è§', 'å…¸å‹'
        ]
        
        # è¯ç‰©åç§°æ¨¡å¼ï¼ˆå¸¸è§è¯ç‰©åç¼€ï¼‰
        self.drug_suffixes = ['ç‰‡', 'èƒ¶å›Š', 'æ³¨å°„æ¶²', 'é¢—ç²’', 'å£æœæ¶²', 'æ “', 'è†', 'æ•£', 'ä¸¸']
        
    def extract_text_from_json(self, json_data: Dict) -> str:
        """ä»JSONæ•°æ®ä¸­æå–æ‰€æœ‰æ–‡æœ¬å†…å®¹"""
        text_parts = []
        
        if 'pdf_info' in json_data:
            for page in json_data['pdf_info']:
                if 'para_blocks' in page:
                    for block in page['para_blocks']:
                        # å¤„ç†æ™®é€šæ–‡æœ¬å—
                        if 'lines' in block:
                            for line in block['lines']:
                                if 'spans' in line:
                                    line_text = []
                                    for span in line['spans']:
                                        if 'content' in span:
                                            content = span['content'].strip()
                                            if content:
                                                line_text.append(content)
                                    if line_text:
                                        text_parts.append(' '.join(line_text))
                        # å¤„ç†åˆ—è¡¨å—
                        elif 'blocks' in block:
                            for sub_block in block['blocks']:
                                if 'lines' in sub_block:
                                    for line in sub_block['lines']:
                                        if 'spans' in line:
                                            line_text = []
                                            for span in line['spans']:
                                                if 'content' in span:
                                                    content = span['content'].strip()
                                                    if content:
                                                        line_text.append(content)
                                            if line_text:
                                                text_parts.append(' '.join(line_text))
        
        return '\n'.join(text_parts)
    
    def extract_html_tables(self, json_data: Dict) -> List[Dict]:
        """æå–HTMLè¡¨æ ¼å†…å®¹"""
        tables = []
        
        if 'pdf_info' in json_data:
            for page in json_data['pdf_info']:
                if 'para_blocks' in page:
                    for block in page['para_blocks']:
                        # å¤„ç†æ™®é€šæ–‡æœ¬å—
                        if 'lines' in block:
                            for line in block['lines']:
                                if 'spans' in line:
                                    for span in line['spans']:
                                        if 'html' in span:
                                            tables.append({
                                                'html': span['html'],
                                                'content': span.get('content', ''),
                                                'page_idx': page.get('page_idx', 0)
                                            })
                        # å¤„ç†åˆ—è¡¨å—
                        elif 'blocks' in block:
                            for sub_block in block['blocks']:
                                if 'lines' in sub_block:
                                    for line in sub_block['lines']:
                                        if 'spans' in line:
                                            for span in line['spans']:
                                                if 'html' in span:
                                                    tables.append({
                                                        'html': span['html'],
                                                        'content': span.get('content', ''),
                                                        'page_idx': page.get('page_idx', 0)
                                                    })
        
        return tables
    
    def parse_chapters(self, text: str) -> List[Dict]:
        """è§£æç« èŠ‚ç»“æ„"""
        chapters = []
        lines = text.split('\n')
        
        current_chapter = None
        current_section = None
        chapter_counter = 0
        section_counter = 0
        
        for i, line in enumerate(lines):
            line = line.strip()
            if not line:
                continue
            
            # åŒ¹é…ç« èŠ‚ï¼ˆæ”¯æŒå¤šç§æ ¼å¼ï¼‰
            chapter_match = self.chapter_pattern.search(line)
            if chapter_match:
                if current_chapter:
                    if current_section:
                        current_chapter['sections'].append(current_section)
                    chapters.append(current_chapter)
                
                chapter_counter += 1
                chapter_name = chapter_match.group(1).strip()
                # æ¸…ç†ç« èŠ‚åç§°ï¼ˆç§»é™¤é¡µç ç­‰ï¼‰
                chapter_name = re.sub(r'//.*$', '', chapter_name).strip()
                
                current_chapter = {
                    'chapter_id': chapter_counter,
                    'chapter_name': chapter_name,
                    'sections': []
                }
                current_section = None
                section_counter = 0
                continue
            
            # åŒ¹é…èŠ‚
            section_match = self.section_pattern.search(line)
            if section_match and current_chapter:
                if current_section:
                    current_chapter['sections'].append(current_section)
                
                section_counter += 1
                section_name = section_match.group(1).strip()
                # æ¸…ç†èŠ‚åç§°ï¼ˆç§»é™¤é¡µç ç­‰ï¼‰
                section_name = re.sub(r'\s+\d+$', '', section_name).strip()
                section_name = re.sub(r'\.\s*$', '', section_name).strip()
                
                current_section = {
                    'section_id': section_counter,
                    'section_name': section_name,
                    'knowledge_points': []
                }
                continue
        
        # æ·»åŠ æœ€åä¸€ä¸ªç« èŠ‚å’ŒèŠ‚
        if current_section and current_chapter:
            current_chapter['sections'].append(current_section)
        if current_chapter:
            chapters.append(current_chapter)
        
        return chapters
    
    def extract_drug_name(self, text: str) -> Optional[str]:
        """æå–è¯ç‰©åç§°"""
        # å¸¸è§è¯ç‰©åç§°æ¨¡å¼
        drug_patterns = [
            r'([A-Za-z\u4e00-\u9fa5]+(?:ç‰‡|èƒ¶å›Š|æ³¨å°„æ¶²|é¢—ç²’|å£æœæ¶²|æ “|è†|æ•£|ä¸¸|é’ |é’¾|ç´ |é†‡|èƒº|é…®|é…¸|ç¢±|é…¯|è‹·))',
            r'([A-Za-z\u4e00-\u9fa5]{2,8}(?:æ›¿ä¸|æ‹‰å”‘|æ´›å°”|åœ°å¹³|æ™®åˆ©|æ²™å¦|ä»–æ±€|è¥¿æ—|éœ‰ç´ |å¤´å­¢))',
        ]
        
        for pattern in drug_patterns:
            match = re.search(pattern, text)
            if match:
                return match.group(1)
        
        return None
    
    def classify_knowledge_type(self, text: str) -> List[str]:
        """åˆ†ç±»çŸ¥è¯†ç‚¹ç±»å‹"""
        types = []
        text_lower = text.lower()
        
        for ktype, keywords in self.knowledge_types.items():
            if any(keyword in text for keyword in keywords):
                types.append(ktype)
        
        return types if types else ['å…¶ä»–']
    
    def calculate_importance(self, text: str) -> int:
        """è®¡ç®—é‡è¦æ€§ç­‰çº§ï¼ˆ1-5æ˜Ÿï¼‰"""
        score = 3  # é»˜è®¤3æ˜Ÿ
        
        # æ ¹æ®å…³é”®è¯åŠ åˆ†
        for keyword in self.high_freq_keywords:
            if keyword in text:
                score += 0.3
        
        # æ ¹æ®æ˜¯å¦æœ‰è¡¨æ ¼å¯¹æ¯”åŠ åˆ†
        if 'å¯¹æ¯”' in text or 'é‡ç‚¹å¼ºåŒ–' in text or 'è¡¨æ ¼' in text:
            score += 1
        
        # æ ¹æ®è®°å¿†å£è¯€åŠ åˆ†
        if 'å·§è®°' in text or 'å£è¯€' in text:
            score += 0.5
        
        # é™åˆ¶åœ¨1-5ä¹‹é—´
        return min(5, max(1, int(score)))
    
    def extract_structured_knowledge(self, text: str, tables: List[Dict]) -> Dict:
        """æå–ç»“æ„åŒ–çŸ¥è¯†ç‚¹"""
        knowledge = {
            'drug_name': self.extract_drug_name(text),
            'knowledge_types': self.classify_knowledge_type(text),
            'importance_level': self.calculate_importance(text),
            'content': text[:500] if len(text) > 500 else text,  # é™åˆ¶é•¿åº¦
            'structured_data': {}
        }
        
        # æå–é€‚åº”è¯
        indications = self._extract_indications(text)
        if indications:
            knowledge['structured_data']['indications'] = indications
        
        # æå–ç¦å¿Œ
        contraindications = self._extract_contraindications(text)
        if contraindications:
            knowledge['structured_data']['contraindications'] = contraindications
        
        # æå–ä¸è‰¯ååº”
        adverse_reactions = self._extract_adverse_reactions(text)
        if adverse_reactions:
            knowledge['structured_data']['adverse_reactions'] = adverse_reactions
        
        # æå–è®°å¿†å£è¯€
        memory_tips = self._extract_memory_tips(text)
        if memory_tips:
            knowledge['structured_data']['memory_tips'] = memory_tips
        
        # ä»è¡¨æ ¼ä¸­æå–ä¿¡æ¯
        if tables:
            knowledge['structured_data']['tables'] = self._parse_tables(tables)
        
        return knowledge
    
    def _extract_indications(self, text: str) -> List[Dict]:
        """æå–é€‚åº”è¯"""
        indications = []
        
        # æŸ¥æ‰¾æµ“åº¦ç›¸å…³çš„é€‚åº”è¯
        pattern = r'(\d+(?:\.\d+)?%)\s*[ï¼š:]\s*(.+?)(?=\n|$)'
        matches = re.findall(pattern, text)
        
        for concentration, usage in matches:
            indications.append({
                'type': 'æµ“åº¦ç‰¹å®š',
                'concentration': concentration,
                'description': usage.strip(),
                'is_high_freq': any(kw in usage for kw in ['é¦–é€‰', 'ä¸€çº¿'])
            })
        
        # æŸ¥æ‰¾åˆ—è¡¨é¡¹
        list_pattern = r'[â‘ -â‘©]\s*(.+?)(?=[â‘¡-â‘©]|$)'
        matches = re.findall(list_pattern, text)
        
        for item in matches:
            if 'é€‚åº”è¯' in text or 'ç”¨äº' in item or 'æ²»ç–—' in item:
                indications.append({
                    'type': 'åˆ—è¡¨é¡¹',
                    'description': item.strip(),
                    'is_high_freq': self._is_high_frequency(item)
                })
        
        return indications
    
    def _extract_contraindications(self, text: str) -> List[str]:
        """æå–ç¦å¿Œ"""
        contraindications = []
        
        # æŸ¥æ‰¾ç¦å¿Œç›¸å…³å†…å®¹
        if 'ç¦å¿Œ' in text or 'ç¦ç”¨' in text or 'æ…ç”¨' in text:
            # æå–åˆ—è¡¨é¡¹
            list_pattern = r'[â‘ -â‘©]\s*([^â‘¡-â‘©]+?)(?=[â‘¡-â‘©]|$)'
            matches = re.findall(list_pattern, text)
            
            for item in matches:
                if any(kw in item for kw in ['ç¦ç”¨', 'æ…ç”¨', 'ç¦å¿Œ', 'ä¸å®œ']):
                    contraindications.append(item.strip())
        
        return contraindications
    
    def _extract_adverse_reactions(self, text: str) -> Dict:
        """æå–ä¸è‰¯ååº”"""
        reactions = {
            'common': [],
            'serious': [],
            'prevention': []
        }
        
        lines = text.split('\n')
        for line in lines:
            line_lower = line.lower()
            
            if any(word in line for word in ['ä¸è‰¯ååº”', 'å‰¯ä½œç”¨', 'æ¯’æ€§']):
                if 'å¸¸è§' in line or 'å¤šè§' in line:
                    reactions['common'].append(line.strip())
                elif 'ä¸¥é‡' in line or 'å±é™©' in line or 'è‡´æ­»' in line:
                    reactions['serious'].append(line.strip())
                elif 'æ³¨æ„' in line or 'é¢„é˜²' in line or 'é¿å…' in line:
                    reactions['prevention'].append(line.strip())
        
        return reactions
    
    def _extract_memory_tips(self, text: str) -> List[str]:
        """æå–è®°å¿†å£è¯€"""
        tips = []
        
        # æŸ¥æ‰¾æ¶¦å¾·å·§è®°
        pattern = r'ã€æ¶¦å¾·å·§è®°ã€‘[ï¼š:]?\s*([^\n]+)'
        matches = re.findall(pattern, text)
        tips.extend(matches)
        
        # æŸ¥æ‰¾å…¶ä»–è®°å¿†å£è¯€
        pattern2 = r'è®°å¿†å£è¯€[ï¼š:]?\s*([^\n]+)'
        matches2 = re.findall(pattern2, text)
        tips.extend(matches2)
        
        return tips
    
    def _parse_tables(self, tables: List[Dict]) -> List[Dict]:
        """è§£æè¡¨æ ¼å†…å®¹"""
        parsed_tables = []
        
        for table in tables:
            html = table.get('html', '')
            if '<table>' in html:
                # ç®€å•æå–è¡¨æ ¼æ–‡æœ¬
                # ç§»é™¤HTMLæ ‡ç­¾
                text = re.sub(r'<[^>]+>', ' ', html)
                text = re.sub(r'\s+', ' ', text).strip()
                
                parsed_tables.append({
                    'type': 'table',
                    'content': text[:1000]  # é™åˆ¶é•¿åº¦
                })
        
        return parsed_tables
    
    def _is_high_frequency(self, text: str) -> bool:
        """åˆ¤æ–­æ˜¯å¦ä¸ºé«˜é¢‘è€ƒç‚¹"""
        return any(keyword in text for keyword in self.high_freq_keywords)
    
    def build_knowledge_tree(self, json_file_path: str) -> Dict:
        """æ„å»ºçŸ¥è¯†æ ‘"""
        print(f"ğŸ“– æ­£åœ¨è¯»å–æ–‡ä»¶: {json_file_path}")
        
        with open(json_file_path, 'r', encoding='utf-8') as f:
            json_data = json.load(f)
        
        print("ğŸ“ æ­£åœ¨æå–æ–‡æœ¬å†…å®¹...")
        text = self.extract_text_from_json(json_data)
        
        print("ğŸ“Š æ­£åœ¨æå–è¡¨æ ¼å†…å®¹...")
        tables = self.extract_html_tables(json_data)
        
        print("ğŸŒ³ æ­£åœ¨è§£æç« èŠ‚ç»“æ„...")
        chapters = self.parse_chapters(text)
        
        print("ğŸ” æ­£åœ¨è¯†åˆ«çŸ¥è¯†ç‚¹...")
        # æŒ‰ç« èŠ‚å’ŒèŠ‚ç»„ç»‡çŸ¥è¯†ç‚¹
        knowledge_tree = {
            'title': 'è¥¿è¯è¯äºŒçŸ¥è¯†ç‚¹ä½“ç³»',
            'chapters': []
        }
        
        # å°†æ–‡æœ¬æŒ‰ç« èŠ‚åˆ†å‰²
        chapter_texts = re.split(r'ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+ç« ', text)
        
        for i, chapter in enumerate(chapters):
            chapter_id = chapter['chapter_id']
            chapter_name = chapter['chapter_name']
            
            print(f"  å¤„ç†ç¬¬{chapter_id}ç« : {chapter_name}")
            
            chapter_data = {
                'chapter_id': f"ç¬¬{chapter_id}ç« ",
                'chapter_name': chapter_name,
                'sections': []
            }
            
            # è·å–è¯¥ç« èŠ‚çš„æ–‡æœ¬å†…å®¹
            chapter_text = chapter_texts[i] if i < len(chapter_texts) else ''
            
            # å¤„ç†æ¯ä¸ªèŠ‚
            for section in chapter['sections']:
                section_id = section['section_id']
                section_name = section['section_name']
                
                print(f"    å¤„ç†ç¬¬{section_id}èŠ‚: {section_name}")
                
                section_data = {
                    'section_id': f"{chapter_id}.{section_id}",
                    'section_name': section_name,
                    'knowledge_points': []
                }
                
                # åœ¨è¯¥èŠ‚çš„æ–‡æœ¬ä¸­æŸ¥æ‰¾çŸ¥è¯†ç‚¹
                # ä½¿ç”¨æ›´çµæ´»çš„åŒ¹é…ç­–ç•¥
                section_keywords = section_name[:5]  # ä½¿ç”¨èŠ‚åç§°çš„å‰å‡ ä¸ªå­—ä½œä¸ºå…³é”®è¯
                
                # å°è¯•å¤šç§åŒ¹é…æ–¹å¼
                section_patterns = [
                    re.compile(rf'{re.escape(section_keywords)}.*?(?=ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+èŠ‚|ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+ç« |$)', re.DOTALL),
                    re.compile(rf'{re.escape(section_name)}.*?(?=ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+èŠ‚|ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+ç« |$)', re.DOTALL),
                ]
                
                section_text = ''
                for pattern in section_patterns:
                    match = pattern.search(chapter_text)
                    if match:
                        section_text = match.group(0)
                        break
                
                # å¦‚æœæ²¡æ‰¾åˆ°ï¼Œå°è¯•åœ¨æ•´ä¸ªæ–‡æœ¬ä¸­æœç´¢
                if not section_text:
                    # æŸ¥æ‰¾åŒ…å«èŠ‚åç§°å…³é”®è¯çš„æ–‡æœ¬æ®µ
                    lines = chapter_text.split('\n')
                    start_idx = -1
                    for idx, line in enumerate(lines):
                        if section_keywords in line:
                            start_idx = idx
                            break
                    
                    if start_idx >= 0:
                        # è·å–åç»­çš„æ–‡æœ¬ï¼Œç›´åˆ°ä¸‹ä¸€ä¸ªèŠ‚æˆ–ç« 
                        end_idx = len(lines)
                        for idx in range(start_idx + 1, len(lines)):
                            if (re.search(r'ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+èŠ‚', lines[idx]) or 
                                re.search(r'ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+ç« ', lines[idx])):
                                end_idx = idx
                                break
                        section_text = '\n'.join(lines[start_idx:end_idx])
                
                if section_text:
                    # æå–çŸ¥è¯†ç‚¹
                    # æŒ‰æ®µè½åˆ†å‰²ï¼ˆä½¿ç”¨å¤šä¸ªæ¢è¡Œæˆ–ç‰¹å®šæ ‡è®°ï¼‰
                    paragraphs = re.split(r'\n{2,}|ã€|è€ƒç‚¹|é¡¹ç›®|å…·ä½“å†…å®¹', section_text)
                    
                    point_id = 1
                    for para in paragraphs:
                        para = para.strip()
                        if len(para) < 50:  # è·³è¿‡å¤ªçŸ­çš„æ®µè½
                            continue
                        
                        # æ£€æŸ¥æ˜¯å¦åŒ…å«çŸ¥è¯†ç‚¹å…³é”®è¯
                        has_keyword = any(kw in para for kw in [
                            'é€‚åº”è¯', 'ç¦å¿Œ', 'ä¸è‰¯ååº”', 'ä¸´åºŠåº”ç”¨', 'ç”¨æ³•', 'ç”¨é‡',
                            'ç›¸äº’ä½œç”¨', 'ä½œç”¨æœºåˆ¶', 'ä½œç”¨ç‰¹ç‚¹', 'å…¸å‹', 'è¯ç‰©',
                            'æ¶¦å¾·å·§è®°', 'è®°å¿†å£è¯€', 'ä½œç”¨', 'æ²»ç–—', 'ç”¨äº', 'é€‚ç”¨äº'
                        ])
                        
                        if has_keyword:
                            # è·å–ç›¸å…³çš„è¡¨æ ¼ï¼ˆç®€åŒ–å¤„ç†ï¼Œä½¿ç”¨æ‰€æœ‰è¡¨æ ¼ï¼‰
                            knowledge = self.extract_structured_knowledge(para, tables)
                            
                            # å¦‚æœæå–åˆ°äº†æœ‰æ•ˆä¿¡æ¯ï¼Œæ·»åŠ åˆ°çŸ¥è¯†ç‚¹åˆ—è¡¨
                            if (knowledge['drug_name'] or 
                                knowledge['knowledge_types'] != ['å…¶ä»–'] or
                                knowledge['structured_data'] or
                                len(para) > 100):  # é•¿æ®µè½ä¹Ÿè®¤ä¸ºæ˜¯çŸ¥è¯†ç‚¹
                                knowledge['point_id'] = f"{chapter_id}.{section_id}.{point_id}"
                                knowledge['point_title'] = self._generate_point_title(para)
                                knowledge['full_content'] = para[:2000]  # ä¿å­˜å®Œæ•´å†…å®¹ï¼ˆé™åˆ¶é•¿åº¦ï¼‰
                                section_data['knowledge_points'].append(knowledge)
                                point_id += 1
                
                # å°†èŠ‚æ·»åŠ åˆ°ç« èŠ‚ä¸­
                if section_data['knowledge_points']:
                    chapter_data['sections'].append(section_data)
            
            # å°†ç« èŠ‚æ·»åŠ åˆ°çŸ¥è¯†æ ‘ä¸­
            if chapter_data['sections']:
                knowledge_tree['chapters'].append(chapter_data)
        
        return knowledge_tree
    
    def _generate_point_title(self, text: str) -> str:
        """ç”ŸæˆçŸ¥è¯†ç‚¹æ ‡é¢˜"""
        # å°è¯•æå–ç¬¬ä¸€å¥è¯ä½œä¸ºæ ‡é¢˜
        sentences = re.split(r'[ã€‚ï¼ï¼Ÿ\n]', text)
        if sentences:
            title = sentences[0].strip()
            if len(title) > 50:
                title = title[:50] + '...'
            return title
        return 'çŸ¥è¯†ç‚¹'
    
    def save_knowledge_tree(self, knowledge_tree: Dict, output_file: str):
        """ä¿å­˜çŸ¥è¯†æ ‘åˆ°JSONæ–‡ä»¶"""
        print(f"\nğŸ’¾ æ­£åœ¨ä¿å­˜çŸ¥è¯†æ ‘åˆ°: {output_file}")
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(knowledge_tree, f, ensure_ascii=False, indent=2)
        
        # ç»Ÿè®¡ä¿¡æ¯
        total_chapters = len(knowledge_tree['chapters'])
        total_sections = sum(len(ch['sections']) for ch in knowledge_tree['chapters'])
        total_points = sum(
            len(sec['knowledge_points'])
            for ch in knowledge_tree['chapters']
            for sec in ch['sections']
        )
        
        print(f"\nâœ… çŸ¥è¯†æ ‘æ„å»ºå®Œæˆï¼")
        print(f"   ğŸ“š ç« èŠ‚æ•°: {total_chapters}")
        print(f"   ğŸ“– èŠ‚æ•°: {total_sections}")
        print(f"   ğŸ¯ çŸ¥è¯†ç‚¹æ•°: {total_points}")
        print(f"   ğŸ’¾ å·²ä¿å­˜åˆ°: {output_file}")


def main():
    import sys
    sys.stdout.reconfigure(encoding='utf-8')
    
    extractor = KnowledgeTreeExtractor()
    
    input_file = r'e:\tiku\shuju\è¥¿è¯è¯äºŒ1-50é¡µ.json'
    output_file = r'e:\tiku\shuju\è¥¿è¯è¯äºŒçŸ¥è¯†æ ‘.json'
    
    print("=" * 60)
    print("å¼€å§‹æ„å»ºçŸ¥è¯†æ ‘...")
    print("=" * 60)
    
    try:
        knowledge_tree = extractor.build_knowledge_tree(input_file)
        extractor.save_knowledge_tree(knowledge_tree, output_file)
        
        print("\nğŸ‰ å¤„ç†å®Œæˆï¼")
        
    except Exception as e:
        print(f"\nâŒ å‘ç”Ÿé”™è¯¯: {e}")
        import traceback
        traceback.print_exc()


if __name__ == '__main__':
    main()
