#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ä»è¥¿è¯è¯äºŒPDFè§£æJSONä¸­æå–çŸ¥è¯†ç‚¹ï¼Œæ„å»ºæ ‘çŠ¶çŸ¥è¯†ä½“ç³»
"""

import json
import re
from typing import Dict, List, Optional
from collections import defaultdict

class KnowledgeTreeBuilder:
    def __init__(self):
        # ç« èŠ‚åŒ¹é…æ¨¡å¼
        self.chapter_pattern = re.compile(r'ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+ç« \s+([^/]+)')
        self.section_pattern = re.compile(r'ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+èŠ‚\s+([^\n]+)')
        
        # çŸ¥è¯†ç‚¹ç±»å‹å…³é”®è¯
        self.knowledge_keywords = [
            'é€‚åº”è¯', 'ç¦å¿Œ', 'ä¸è‰¯ååº”', 'ä¸´åºŠåº”ç”¨', 'ç”¨æ³•', 'ç”¨é‡',
            'ç›¸äº’ä½œç”¨', 'ä½œç”¨æœºåˆ¶', 'ä½œç”¨ç‰¹ç‚¹', 'å…¸å‹', 'è¯ç‰©',
            'æ¶¦å¾·å·§è®°', 'è®°å¿†å£è¯€', 'ä½œç”¨', 'æ²»ç–—', 'ç”¨äº', 'é€‚ç”¨äº',
            'è¯ç‰©', 'è¯å“', 'ä»£è¡¨', 'åˆ†ç±»', 'ç‰¹ç‚¹', 'æ³¨æ„'
        ]
        
        # é«˜é¢‘è€ƒç‚¹å…³é”®è¯
        self.high_freq_keywords = [
            'é¦–é€‰', 'ä¸€çº¿', 'é‡‘æ ‡å‡†', 'æœ€å¸¸ç”¨', 'ä¸»è¦', 'é‡è¦',
            'ä¸¥é‡', 'ç¦å¿Œ', 'ç¦ç”¨', 'ç‰¹åˆ«æ³¨æ„', 'é‡ç‚¹',
            'ç›¸äº’ä½œç”¨', 'é…ä¼ç¦å¿Œ', 'å¸¸è§', 'å…¸å‹'
        ]
    
    def extract_all_text(self, json_data: Dict) -> str:
        """ä»JSONæ•°æ®ä¸­æå–æ‰€æœ‰æ–‡æœ¬å†…å®¹"""
        text_parts = []
        
        if 'pdf_info' in json_data:
            for page in json_data['pdf_info']:
                if 'para_blocks' in page:
                    for block in page['para_blocks']:
                        if 'lines' in block:
                            for line in block['lines']:
                                if 'spans' in line:
                                    line_text = []
                                    for span in line['spans']:
                                        if 'content' in span:
                                            content = span['content'].strip()
                                            if content:
                                                line_text.append(content)
                                    if line_text:
                                        text_parts.append(' '.join(line_text))
        
        return '\n'.join(text_parts)
    
    def extract_html_tables(self, json_data: Dict) -> List[Dict]:
        """æå–HTMLè¡¨æ ¼å†…å®¹"""
        tables = []
        
        if 'pdf_info' in json_data:
            for page_idx, page in enumerate(json_data['pdf_info']):
                if 'para_blocks' in page:
                    for block in page['para_blocks']:
                        if 'lines' in block:
                            for line in block['lines']:
                                if 'spans' in line:
                                    for span in line['spans']:
                                        if 'html' in span:
                                            tables.append({
                                                'html': span['html'],
                                                'page_idx': page_idx
                                            })
        
        return tables
    
    def parse_structure(self, text: str) -> List[Dict]:
        """è§£æç« èŠ‚å’ŒèŠ‚çš„ç»“æ„"""
        chapters = []
        lines = text.split('\n')
        
        current_chapter = None
        current_section = None
        chapter_counter = 0
        section_counter = 0
        
        for line in lines:
            line = line.strip()
            if not line:
                continue
            
            # åŒ¹é…ç« èŠ‚
            chapter_match = self.chapter_pattern.search(line)
            if chapter_match:
                if current_chapter:
                    if current_section:
                        current_chapter['sections'].append(current_section)
                    chapters.append(current_chapter)
                
                chapter_counter += 1
                chapter_name = chapter_match.group(1).strip()
                chapter_name = re.sub(r'//.*$', '', chapter_name).strip()
                
                current_chapter = {
                    'chapter_id': chapter_counter,
                    'chapter_name': chapter_name,
                    'sections': []
                }
                current_section = None
                section_counter = 0
                continue
            
            # åŒ¹é…èŠ‚
            section_match = self.section_pattern.search(line)
            if section_match and current_chapter:
                if current_section:
                    current_chapter['sections'].append(current_section)
                
                section_counter += 1
                section_name = section_match.group(1).strip()
                section_name = re.sub(r'\s+\d+$', '', section_name).strip()
                section_name = re.sub(r'\.\s*$', '', section_name).strip()
                
                current_section = {
                    'section_id': section_counter,
                    'section_name': section_name,
                    'start_line': line
                }
                continue
        
        # æ·»åŠ æœ€åä¸€ä¸ª
        if current_section and current_chapter:
            current_chapter['sections'].append(current_section)
        if current_chapter:
            chapters.append(current_chapter)
        
        return chapters
    
    def extract_knowledge_points(self, text: str, section_name: str, section_start_idx: int) -> List[Dict]:
        """ä»æ–‡æœ¬ä¸­æå–çŸ¥è¯†ç‚¹"""
        knowledge_points = []
        lines = text.split('\n')
        
        # æ‰¾åˆ°èŠ‚å¼€å§‹ä½ç½®
        section_text_start = section_start_idx
        section_text_end = len(lines)
        
        # æŸ¥æ‰¾ä¸‹ä¸€ä¸ªèŠ‚æˆ–ç« çš„å¼€å§‹
        for i in range(section_start_idx + 1, len(lines)):
            if (self.section_pattern.search(lines[i]) or 
                self.chapter_pattern.search(lines[i])):
                section_text_end = i
                break
        
        # æå–è¯¥èŠ‚çš„æ–‡æœ¬
        section_text = '\n'.join(lines[section_text_start:section_text_end])
        
        # æŒ‰æ®µè½åˆ†å‰²
        paragraphs = re.split(r'\n{2,}|ã€|è€ƒç‚¹|é¡¹ç›®|å…·ä½“å†…å®¹|ä½œç”¨ç‰¹ç‚¹|å…¸å‹ä¸è‰¯ååº”|ç¦å¿Œ|è¯ç‰©ç›¸äº’ä½œç”¨', section_text)
        
        point_id = 1
        for para in paragraphs:
            para = para.strip()
            if len(para) < 50:  # è·³è¿‡å¤ªçŸ­çš„æ®µè½
                continue
            
            # æ£€æŸ¥æ˜¯å¦åŒ…å«çŸ¥è¯†ç‚¹å…³é”®è¯
            has_keyword = any(kw in para for kw in self.knowledge_keywords)
            
            if has_keyword:
                # æå–è¯ç‰©åç§°ï¼ˆç®€å•æ¨¡å¼ï¼‰
                drug_name = None
                drug_patterns = [
                    r'([A-Za-z\u4e00-\u9fa5]{2,8}(?:æ›¿ä¸|æ‹‰å”‘|æ´›å°”|åœ°å¹³|æ™®åˆ©|æ²™å¦|ä»–æ±€|è¥¿æ—|éœ‰ç´ |å¤´å­¢|ç´¢|é†‡|èƒº|é…®|é…¸|ç¢±|é…¯|è‹·|é’ |é’¾))',
                    r'([\u4e00-\u9fa5]{2,6}(?:ç‰‡|èƒ¶å›Š|æ³¨å°„æ¶²|é¢—ç²’|å£æœæ¶²))',
                ]
                for pattern in drug_patterns:
                    match = re.search(pattern, para)
                    if match:
                        drug_name = match.group(1)
                        break
                
                # åˆ†ç±»çŸ¥è¯†ç‚¹ç±»å‹
                knowledge_types = []
                if 'é€‚åº”è¯' in para or 'ç”¨äº' in para or 'æ²»ç–—' in para:
                    knowledge_types.append('é€‚åº”è¯')
                if 'ç¦å¿Œ' in para or 'ç¦ç”¨' in para or 'æ…ç”¨' in para:
                    knowledge_types.append('ç¦å¿Œ')
                if 'ä¸è‰¯ååº”' in para or 'å‰¯ä½œç”¨' in para:
                    knowledge_types.append('ä¸è‰¯ååº”')
                if 'ç›¸äº’ä½œç”¨' in para or 'åˆç”¨' in para or 'è”ç”¨' in para:
                    knowledge_types.append('è¯ç‰©ç›¸äº’ä½œç”¨')
                if 'ä½œç”¨æœºåˆ¶' in para or 'ä½œç”¨ç‰¹ç‚¹' in para:
                    knowledge_types.append('ä½œç”¨æœºåˆ¶')
                if 'æ¶¦å¾·å·§è®°' in para or 'è®°å¿†å£è¯€' in para or 'å·§è®°' in para:
                    knowledge_types.append('è®°å¿†å£è¯€')
                if not knowledge_types:
                    knowledge_types.append('å…¶ä»–')
                
                # è®¡ç®—é‡è¦æ€§
                importance = 3
                for keyword in self.high_freq_keywords:
                    if keyword in para:
                        importance += 0.3
                if 'å¯¹æ¯”' in para or 'é‡ç‚¹å¼ºåŒ–' in para:
                    importance += 1
                if 'å·§è®°' in para or 'å£è¯€' in para:
                    importance += 0.5
                importance = min(5, max(1, int(importance)))
                
                # ç”Ÿæˆæ ‡é¢˜
                sentences = re.split(r'[ã€‚ï¼ï¼Ÿ\n]', para)
                title = sentences[0].strip() if sentences else 'çŸ¥è¯†ç‚¹'
                if len(title) > 50:
                    title = title[:50] + '...'
                
                knowledge_point = {
                    'point_id': f"{point_id}",
                    'point_title': title,
                    'drug_name': drug_name,
                    'knowledge_types': knowledge_types,
                    'importance_level': importance,
                    'content': para[:1000],  # é™åˆ¶é•¿åº¦
                    'full_content': para[:2000]
                }
                
                knowledge_points.append(knowledge_point)
                point_id += 1
        
        return knowledge_points
    
    def build_tree(self, json_file_path: str, output_file: str):
        """æ„å»ºçŸ¥è¯†æ ‘"""
        print("=" * 60)
        print("å¼€å§‹æ„å»ºçŸ¥è¯†æ ‘...")
        print("=" * 60)
        
        # è¯»å–JSONæ–‡ä»¶
        print(f"ğŸ“– æ­£åœ¨è¯»å–æ–‡ä»¶: {json_file_path}")
        with open(json_file_path, 'r', encoding='utf-8') as f:
            json_data = json.load(f)
        
        # æå–æ–‡æœ¬
        print("ğŸ“ æ­£åœ¨æå–æ–‡æœ¬å†…å®¹...")
        text = self.extract_all_text(json_data)
        print(f"   æå–äº† {len(text)} ä¸ªå­—ç¬¦")
        
        # æå–è¡¨æ ¼
        print("ğŸ“Š æ­£åœ¨æå–è¡¨æ ¼å†…å®¹...")
        tables = self.extract_html_tables(json_data)
        print(f"   æ‰¾åˆ° {len(tables)} ä¸ªè¡¨æ ¼")
        
        # è§£æç»“æ„
        print("ğŸŒ³ æ­£åœ¨è§£æç« èŠ‚ç»“æ„...")
        chapters = self.parse_structure(text)
        print(f"   æ‰¾åˆ° {len(chapters)} ä¸ªç« èŠ‚")
        
        # æ„å»ºçŸ¥è¯†æ ‘
        print("ğŸ” æ­£åœ¨è¯†åˆ«çŸ¥è¯†ç‚¹...")
        knowledge_tree = {
            'title': 'è¥¿è¯è¯äºŒçŸ¥è¯†ç‚¹ä½“ç³»',
            'total_chapters': len(chapters),
            'total_sections': 0,
            'total_points': 0,
            'chapters': []
        }
        
        lines = text.split('\n')
        
        for chapter in chapters:
            chapter_id = chapter['chapter_id']
            chapter_name = chapter['chapter_name']
            
            print(f"  å¤„ç†ç¬¬{chapter_id}ç« : {chapter_name}")
            
            chapter_data = {
                'chapter_id': f"ç¬¬{chapter_id}ç« ",
                'chapter_name': chapter_name,
                'sections': []
            }
            
            # æ‰¾åˆ°ç« èŠ‚å¼€å§‹ä½ç½®
            chapter_start_idx = -1
            for idx, line in enumerate(lines):
                if f"ç¬¬{chapter_id}ç« " in line or chapter_name[:5] in line:
                    chapter_start_idx = idx
                    break
            
            for section in chapter['sections']:
                section_id = section['section_id']
                section_name = section['section_name']
                
                print(f"    å¤„ç†ç¬¬{section_id}èŠ‚: {section_name}")
                
                # æ‰¾åˆ°èŠ‚å¼€å§‹ä½ç½®
                section_start_idx = -1
                for idx in range(chapter_start_idx if chapter_start_idx >= 0 else 0, len(lines)):
                    if section_name[:5] in lines[idx]:
                        section_start_idx = idx
                        break
                
                if section_start_idx < 0:
                    section_start_idx = chapter_start_idx if chapter_start_idx >= 0 else 0
                
                # æå–çŸ¥è¯†ç‚¹
                knowledge_points = self.extract_knowledge_points(
                    text, section_name, section_start_idx
                )
                
                section_data = {
                    'section_id': f"{chapter_id}.{section_id}",
                    'section_name': section_name,
                    'knowledge_points': knowledge_points,
                    'point_count': len(knowledge_points)
                }
                
                chapter_data['sections'].append(section_data)
                knowledge_tree['total_points'] += len(knowledge_points)
            
            knowledge_tree['chapters'].append(chapter_data)
            knowledge_tree['total_sections'] += len(chapter_data['sections'])
        
        # ä¿å­˜
        print(f"\nğŸ’¾ æ­£åœ¨ä¿å­˜çŸ¥è¯†æ ‘åˆ°: {output_file}")
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(knowledge_tree, f, ensure_ascii=False, indent=2)
        
        print(f"\nâœ… çŸ¥è¯†æ ‘æ„å»ºå®Œæˆï¼")
        print(f"   ğŸ“š ç« èŠ‚æ•°: {knowledge_tree['total_chapters']}")
        print(f"   ğŸ“– èŠ‚æ•°: {knowledge_tree['total_sections']}")
        print(f"   ğŸ¯ çŸ¥è¯†ç‚¹æ•°: {knowledge_tree['total_points']}")
        print(f"   ğŸ’¾ å·²ä¿å­˜åˆ°: {output_file}")
        print("\nğŸ‰ å¤„ç†å®Œæˆï¼")
        
        return knowledge_tree


def main():
    builder = KnowledgeTreeBuilder()
    
    input_file = r'e:\tiku\shuju\è¥¿è¯è¯äºŒ1-50é¡µ.json'
    output_file = r'e:\tiku\shuju\è¥¿è¯è¯äºŒçŸ¥è¯†æ ‘.json'
    
    try:
        knowledge_tree = builder.build_tree(input_file, output_file)
    except Exception as e:
        print(f"\nâŒ å‘ç”Ÿé”™è¯¯: {e}")
        import traceback
        traceback.print_exc()


if __name__ == '__main__':
    main()
